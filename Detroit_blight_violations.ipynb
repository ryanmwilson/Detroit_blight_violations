{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rwilson/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random as rdm\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# load latitude/longitude/address data\n",
    "latlons = pd.read_csv('latlons.csv', encoding = 'ISO-8859-1', skiprows=None, low_memory=False)\n",
    "addresses = pd.read_csv('addresses.csv', encoding = 'ISO-8859-1', skiprows=None, low_memory=False)\n",
    "dfmap = addresses.merge(latlons, left_on='address', right_on='address', how='inner')\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv('train.csv', encoding = 'ISO-8859-1', skiprows=None, low_memory=False)\n",
    "df = df[~np.isnan(df['compliance'])] # removing empty output values\n",
    "df = df.merge(dfmap,left_on='ticket_id', right_on='ticket_id', how='inner')\n",
    "df.reset_index(inplace=True)\n",
    "df = df.drop(['index'],1)\n",
    "\n",
    "# load testing data\n",
    "dftest = pd.read_csv('test.csv', encoding = 'ISO-8859-1', skiprows=None, low_memory=False)\n",
    "dftest = dftest.merge(dfmap,left_on='ticket_id', right_on='ticket_id', how='inner')\n",
    "dftest.reset_index(inplace=True)\n",
    "dftest = dftest.drop(['index'],1)\n",
    "\n",
    "# extract outputs (can be removed for assignment test)\n",
    "y = df['compliance']\n",
    "df = df.drop('compliance',1);\n",
    "\n",
    "# clean data\n",
    "\n",
    "le_d = le.fit(dftest['disposition'].unique())\n",
    "df['disposition'] = le_d.transform(df['disposition'])\n",
    "df['violation_code'] = df['violation_code'].apply(lambda x: re.sub('[^0-9]', '', x))\n",
    "df['zip_code'] = df['zip_code'].str[:5]\n",
    "df['zip_code'] = pd.to_numeric(df['zip_code'], errors='coerce')\n",
    "\n",
    "# determine whether violation and mailing addresses match\n",
    "df['violation_street_name'] = df['violation_street_name'].astype(str)\n",
    "df['mailing_address_str_name'] = df['mailing_address_str_name'].astype(str)\n",
    "df['violation_street_name'] = df['violation_street_name'].str.replace('[^\\w\\s]','').str.replace(' st', '').str.replace(' ave', '').str.replace(' blvd', '')\n",
    "df['mailing_address_str_name'] = df['mailing_address_str_name'].str.replace('[^\\w\\s]','').str.replace(' st', '').str.replace(' ave', '').str.replace(' blvd', '')\n",
    "df['violation_street_name'] = df['violation_street_name'].apply(lambda x: x.lower())\n",
    "df['mailing_address_str_name'] = df['mailing_address_str_name'].apply(lambda x: x.lower())\n",
    "df['same_address'] = df.apply(lambda row: SequenceMatcher( None, str(row['violation_street_name']), str(row['mailing_address_str_name'])).ratio(), axis=1) >= 0.75\n",
    "df['same_address'] = df['same_address'].astype(int)\n",
    "\n",
    "# date of issue, hearing date \n",
    "df['ticket_issued_date'] = pd.to_datetime(df['ticket_issued_date'])\n",
    "df['hearing_date'] = pd.to_datetime(df['hearing_date'])\n",
    "df['time_delta'] = (pd.to_datetime(df['hearing_date']).dt.date - pd.to_datetime(df['ticket_issued_date']).dt.date).astype('timedelta64[D]').astype(float)\n",
    "df['time_delta'][df['time_delta']<0] += 365\n",
    "df['day_issued'] = df['ticket_issued_date'].dt.day.astype(float)\n",
    "df['month_issued'] = df['ticket_issued_date'].dt.month.astype(float)\n",
    "df['hearing_day'] = df['hearing_date'].dt.weekday.astype(float)\n",
    "\n",
    "# is time delta null?\n",
    "df['dtnull'] = df['time_delta'].isnull().astype(int)\n",
    "\n",
    "# drop features that are not available in the test data\n",
    "dropcolumnstrain = ['payment_amount','payment_date','payment_status','payment_status','balance_due','collection_status','compliance_detail']\n",
    "df = df.drop(dropcolumnstrain, 1)\n",
    "\n",
    "# drop irrelevant features (common for both training and test)\n",
    "dropcolumns = ['country','state','ticket_issued_date','hearing_date','city','violation_street_number','violation_street_name','mailing_address_str_name','mailing_address_str_number','mailing_address_str_name','address','agency_name','inspector_name','ticket_id','violator_name','violation_zip_code','admin_fee','state_fee','violation_description','clean_up_cost','grafitti_status','judgment_amount','non_us_str_code']\n",
    "df = df.drop(dropcolumns, 1)\n",
    "\n",
    "df = df.astype(float)\n",
    "\n",
    "# create train and test sets\n",
    "df, dftest, y_train, y_test = train_test_split(df, y, random_state=0)\n",
    "\n",
    "# now do cleaning that references other data (must split train/test first!)\n",
    "df['zip_code'] = df['zip_code'].fillna(df['zip_code'].value_counts().idxmax())\n",
    "df['lat'] = df['lat'].fillna(df['lat'].value_counts().mean())\n",
    "df['lon'] = df['lon'].fillna(df['lon'].value_counts().mean())\n",
    "df['time_delta'] = df['time_delta'].fillna(df['time_delta'].value_counts().mean())\n",
    "df['hearing_day'] = df['hearing_day'].fillna(df['hearing_day'].value_counts().idxmax())\n",
    "\n",
    "X_train = df;\n",
    "\n",
    "dftest['zip_code'] = dftest['zip_code'].fillna(dftest['zip_code'].value_counts().idxmax())\n",
    "dftest['lat'] = dftest['lat'].fillna(dftest['lat'].value_counts().mean())\n",
    "dftest['lon'] = dftest['lon'].fillna(dftest['lon'].value_counts().mean())\n",
    "dftest['time_delta'] = dftest['time_delta'].fillna(dftest['time_delta'].value_counts().mean())\n",
    "dftest['hearing_day'] = dftest['hearing_day'].fillna(dftest['hearing_day'].value_counts().idxmax())\n",
    "\n",
    "X_test = dftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.835327687729\n",
      "{'n_estimators': 200, 'max_features': 2, 'max_depth': 20}\n"
     ]
    }
   ],
   "source": [
    "# random forest grid search\n",
    "# run to search for best random forest parameters\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV\n",
    "\n",
    "# clf = RandomForestClassifier(max_features = 3, n_estimators = 5, random_state = 0)\n",
    "clf = RandomForestClassifier(random_state = 0).fit(X_train, y_train)\n",
    "\n",
    "grid_values = {'n_estimators': [100,200], 'max_depth': [10,20,30], 'max_features': [2,5,10]}\n",
    "\n",
    "grid_clf_auc = GridSearchCV(clf, param_grid = grid_values, scoring = 'roc_auc')\n",
    "grid_clf_auc.fit(X_train, y_train)\n",
    "\n",
    "print(grid_clf_auc.best_score_)\n",
    "print(grid_clf_auc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84375871261488522"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run random forest classifier with optimized parameters, outputs ROC AUC score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV\n",
    "\n",
    "clf = RandomForestClassifier(random_state = 0, max_depth = 20, n_estimators = 100, max_features = 2, criterion='entropy', n_jobs = -1).fit(X_train, y_train)\n",
    "roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-machine-learning",
   "graded_item_id": "nNS8l",
   "launcher_item_id": "yWWk7",
   "part_id": "w8BSS"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
